[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# AI-Powered Citation Verification

A web app for citation verification.
This app was developed as part of an [ML capstone project](https://github.com/jedick/ML-capstone-project).

- Enter a claim and evidence statements to verify the claim
- The app runs the model and outputs a classification: Support, Refute, or Not Enough Information (NEI)
  - The [default model](https://huggingface.co/jedick/DeBERTa-v3-base-mnli-fever-anli-scifact-citint) was fine-tuned on two datasets, SciFact and Citation-Integrity
  - The [base model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) is DeBERTa pre-trained on multiple natural language inference (NLI) datasets
  - See the [blog post](https://jedick.github.io/blog/experimenting-with-transformer-models-for-citation-verification/) for more information on fine-tuning
- App built with [Gradio](https://github.com/gradio-app/gradio)

## Running the app

Install the requirements with the `pip install` command below.
Then, run the `python` command and open the generated URL to launch the app.

```
pip install transformers gradio
python app.py
```

App usage:

- Input a claim and evidence (aka hypothesis and evidence in NLI datasets)
- Hit "Enter" or press the Submit button to run the inference
- The model output is printed in the Prediction text box and visualized in the barchart
- Change the model using the dropdown at the top
  - This automatically re-runs the inference using the selected model

Screenshot of app with [example text](https://huggingface.co/datasets/nyu-mll/multi_nli/viewer/default/train?row=37&views%5B%5D=train):

![Screenshot of AI4citations app](./images/AI4citations_screenshot.png)

## Evals

Predictions were made on the SciFact test set.

- *Gold evidence* is the abstract from the cited paper used by human annotators to label each claim
- *Retrieved evidence* is sentences retrieved from the PDF of the cited paper using [BM25S](https://github.com/xhluca/bm25s)
  - The claim was used as the query
  - Sentences were only retrieved from the cited PDF
  - The number of retrieved sentences (top k) was set to 5 or 10

Results:

- Macro F1 with gold evidence: 0.834
- Macro F1 with retrieved evidence (k=5): 0.692
- Macro F1 with retrieved evidence (k=10): 0.646

